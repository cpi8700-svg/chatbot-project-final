import streamlit as st
import google.generativeai as genai
import os
import time

st.set_page_config(page_title="í˜„ì¥ ì—…ë¬´ ì§€ì›", page_icon="ğŸ‘·")
st.title("ğŸ‘· í˜„ì¥ ì—…ë¬´/ì œí’ˆ ì§€ì›")

# --- [1. API í‚¤ ë¶ˆëŸ¬ì˜¤ê¸°] ---
if "api_key" not in st.session_state or not st.session_state["api_key"]:
    if "GOOGLE_API_KEY" in st.secrets:
        st.session_state["api_key"] = st.secrets["GOOGLE_API_KEY"]
    else:
        st.warning("âš ï¸ ë©”ì¸ í˜ì´ì§€ì—ì„œ API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        st.stop()

# --- [2. AI ë¹„ì„œ ì„¤ì •] ---
try:
    genai.configure(api_key=st.session_state["api_key"])
except Exception as e:
    st.error(f"API í‚¤ ì„¤ì • ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    st.stop()

# ëª¨ë¸ ì„¤ì • (2.0 ì‹œë„ -> 1.5 ìë™ ì „í™˜)
try:
    model = genai.GenerativeModel('gemini-3-flash-preview')
except:
    model = genai.GenerativeModel('gemini-3-flash-preview')

# -----------------------------------------------------------
# ğŸ”¥ [í•µì‹¬ ê¸°ëŠ¥] VS Codeì— ìˆëŠ” 'manual.pdf' ìë™ ë¡œë”©
# -----------------------------------------------------------
@st.cache_resource  # (ì¤‘ìš”) í•œ ë²ˆ ì½ìœ¼ë©´ ê³„ì† ê¸°ì–µí•˜ê²Œ ë§Œë“¦
def load_local_manual():
    # 1. íŒŒì¼ ì´ë¦„ì´ ì •í™•í•œì§€ í™•ì¸í•˜ì„¸ìš”! (manual.pdf)
    file_path = "manual.pdf" 
    
    if os.path.exists(file_path):
        try:
            # êµ¬ê¸€ ì„œë²„ë¡œ ì—…ë¡œë“œ
            uploaded_file = genai.upload_file(file_path)
            
            # íŒŒì¼ ì²˜ë¦¬ë  ë•Œê¹Œì§€ ëŒ€ê¸°
            while uploaded_file.state.name == "PROCESSING":
                time.sleep(1)
                uploaded_file = genai.get_file(uploaded_file.name)
                
            return uploaded_file
        except Exception as e:
            st.error(f"ë§¤ë‰´ì–¼ ë¡œë”© ì‹¤íŒ¨: {e}")
            return None
    else:
        return None

# í•¨ìˆ˜ ì‹¤í–‰í•´ì„œ íŒŒì¼ ê°€ì ¸ì˜¤ê¸°
default_manual = load_local_manual()

# -----------------------------------------------------------

# --- [3. ì±„íŒ… í™”ë©´ ë§Œë“¤ê¸°] ---
if default_manual:
    st.success("âœ… 'manual.pdf' ë§¤ë‰´ì–¼ì´ ì •ìƒì ìœ¼ë¡œ íƒ‘ì¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
else:
    st.info("ğŸ’¡ ë“±ë¡ëœ ê¸°ë³¸ ë§¤ë‰´ì–¼ì´ ì—†ìŠµë‹ˆë‹¤. (ê´€ë¦¬ì í˜ì´ì§€ì—ì„œ ì¶”ê°€ ê°€ëŠ¥)")


# ì±„íŒ… ê¸°ë¡ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# ì´ì „ ëŒ€í™” ë‚´ìš© ë³´ì—¬ì£¼ê¸°
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì§ˆë¬¸ ì…ë ¥ ë°›ê¸°
if prompt := st.chat_input("ì§ˆë¬¸ ì…ë ¥ (ì˜ˆ: ì‹œê¸‰ì€?)"):
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        
        try:
            # í•™ìŠµ ìë£Œ ëª¨ìœ¼ê¸° (ê¸°ë³¸ ë§¤ë‰´ì–¼ + ê´€ë¦¬ìê°€ ì¶”ê°€ë¡œ ì˜¬ë¦° ê±°)
            content_to_send = [prompt]
            
            # 1. VS Codeì— ë°•ì•„ë‘” ê¸°ë³¸ ë§¤ë‰´ì–¼ ì¶”ê°€
            if default_manual:
                content_to_send.append(default_manual)
            
            # 2. ê´€ë¦¬ì í˜ì´ì§€ì—ì„œ ì„ì‹œë¡œ ì˜¬ë¦° íŒŒì¼ ì¶”ê°€
            if "uploaded_files_cache" in st.session_state and st.session_state["uploaded_files_cache"]:
                content_to_send.extend(st.session_state["uploaded_files_cache"])
            
            # ìë£Œê°€ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ "ë§¤ë‰´ì–¼ ë³´ëŠ” ì¤‘" í‘œì‹œ
            if len(content_to_send) > 1:
                message_placeholder.markdown("ğŸ“˜ ë§¤ë‰´ì–¼ ë‚´ìš©ì„ í™•ì¸í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
            
            # AIì—ê²Œ ì§ˆë¬¸ ë˜ì§€ê¸°
            response = model.generate_content(content_to_send)
            full_response = response.text
            message_placeholder.markdown(full_response)
            
            st.session_state.messages.append({"role": "assistant", "content": full_response})
            
        except Exception as e:
            st.error("ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            st.caption(f"ì—ëŸ¬ ë‚´ìš©: {e}")